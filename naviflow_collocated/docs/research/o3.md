Great. I’ll prepare a detailed markdown report guiding the design and implementation of a modular, mesh-agnostic, object-oriented Python SIMPLE solver for steady incompressible lid-driven cavity flow, following Moukalled’s finite volume methodology.

The report will cover:

* Mathematical formulations per SIMPLE step
* Design patterns for collocated FVM on structured/unstructured meshes
* Implementation insights compatible with Numba, MPI, and PETSc (parallel-ready, GPU-aware)
* Suggestions for extensibility to transient solvers like PISO

I’ll also align discretization to use the power-law scheme as baseline, with considerations for QUICK and TVD later. I’ll let you know when it’s ready.


# Python-Based SIMPLE Solver for Steady Incompressible Lid-Driven Cavity Flow

## Introduction and Problem Setup

* **Governing Equations:** We solve the steady incompressible Navier–Stokes equations – continuity `∇·u = 0` and momentum `ρ (u·∇)u = -∇p + μ∇²u` – for a 2D lid-driven cavity flow. All variables (velocity components **u** and pressure **p**) are stored at cell centers in a *collocated* finite-volume mesh arrangement. This collocated scheme avoids the bookkeeping of staggered grids but requires special treatment to prevent pressure–velocity decoupling (checkerboard patterns).
* **Finite Volume Discretization:** The domain is divided into control volumes (cells), and the governing equations are integrated over each cell. We obtain algebraic equations by applying Gauss’s divergence theorem on fluxes through cell faces. Convective fluxes are obtained via interpolation of cell-center values to faces, and diffusive fluxes via gradients at faces, ensuring conservative flux balance for each cell.
* **Boundary Conditions:** In the lid-driven cavity, no-slip walls bound the domain; the top lid moves with a specified velocity. Velocity boundary conditions are implemented as Dirichlet conditions on the momentum equations (e.g. top wall u = U\_lid, others u = 0). Pressure is not directly specified (for a closed cavity, pressure is determined up to a constant), so a reference pressure (e.g. p = 0 at a corner) is set to anchor the solution.
* **Pressure–Velocity Coupling Challenge:** In incompressible flow there is no independent pressure equation; pressure emerges as a Lagrange multiplier enforcing continuity. A *pressure-correction* algorithm is needed. We adopt the classical **SIMPLE** (Semi-Implicit Method for Pressure-Linked Equations) iterative scheme of Patankar and Spalding, as outlined by Moukalled *et al.*, to enforce mass conservation by sequentially solving momentum and pressure correction equations. Under-relaxation is used to stabilize these iterations.

## SIMPLE Algorithm Overview

* **Segregated Iteration:** The solver performs a Picard-type iterative loop. Starting from an initial guess, each iteration executes:

  1. **Momentum Solve (Velocity Prediction):** Solve the momentum equations using the current pressure field (from the previous iteration) to obtain an intermediate velocity field **u**\* that satisfies momentum conservation but not necessarily continuity.
  2. **Pressure Correction Solve:** Formulate and solve a pressure-correction equation derived from the continuity constraint and momentum discretization. This yields a pressure correction **p’** field that, when applied, will adjust the pressure and velocity to enforce divergence-free flow.
  3. **Velocity/Pressure Update:** Correct the cell-centered velocities and pressures: **u := u* + u’*\* and **p := p + p’**, such that the updated fields satisfy continuity. Optionally apply under-relaxation factors to these updates for stability.
  4. **Convergence Check:** Compute residuals (e.g. momentum equation residuals and maximum continuity error). If not converged, repeat the steps with the new pressure and velocity fields.
* **Under-Relaxation:** To ensure stable convergence, especially at higher Reynolds numbers or coarse grids, under-relaxation is applied:

  * After solving momentum, the updated velocity can be blended with the old velocity: **u**\* = α\_u \* u\_new + (1-α\_u) \* u\_old (implicitly achieved by adding \$(1-α\_u)\$ fraction of old value to diagonal of momentum matrix). Similarly, the pressure update uses **p := p + α\_p \* p’**. Typical values are α\_u \~0.7–0.9 for momentum and a smaller α\_p \~0.3–0.7 for pressure.
  * Under-relaxation prevents overshooting and ensures the coefficient matrix remains diagonally dominant. It effectively damps oscillations, improving stability at the cost of more iterations.
* **Collocated Grid and Rhie–Chow:** A collocated arrangement requires computing face velocities carefully to avoid decoupling. The SIMPLE loop uses Rhie–Chow momentum interpolation when assembling the pressure correction step (details in the pressure correction section). This ensures that the pressure and velocity fields remain coupled, eliminating unphysical checkerboard pressure patterns on collocated meshes.

## Momentum Equation Discretization and Solution (Predictor Step)

**Mathematical Considerations:**

* **Finite Volume Momentum Equations:** We discretize the steady momentum equations for each cell *P* in x and y directions. For example, the x-momentum integrated over cell *P* yields: \$a\_P u\_P = \sum\_{N} a\_{N} u\_{N} - V\_P \frac{\partial p}{\partial x}\big|\_P + S\_P\$, where *N* denotes neighboring cells (east, west, north, south in 2D) and \$V\_P\$ is cell volume. The term \$-V\_P (\partial p/\partial x)\_P\$ comes from pressure gradient integration, and \$S\_P\$ represents any source term (zero here aside from pressure and possibly body forces). A similar equation exists for y-momentum. The coefficients \$a\_P\$ and \$a\_N\$ arise from integrating convection and diffusion terms:

  * **Diffusion:** Using central differencing, the diffusive flux through a face is \$(\mu A\_f/d) (u\_N - u\_P)\$, leading to symmetric contributions to \$a\_P\$ and \$a\_N\$. This yields a standard 5-point stencil for Laplacian in 2D (each neighbor contributes to the central coefficient) ensuring diagonal dominance (diffusion coefficients are positive and dominate for fine grids).
  * **Convection:** We use an upwind scheme as default for stability: the convective flux at face *f* uses the velocity of the upwind cell. E.g. \$F\_e = \rho u\_e A\_e\$ with \$u\_e\$ taken as \$u\_P\$ or \$u\_E\$ depending on the flow direction. This yields \$a\_N\$ coefficients that are non-symmetric and introduce numerical diffusion but ensure boundedness (coefficient contributions remain positive). For low-Re flows, second-order central differencing can be used for convection to reduce numerical diffusion, but one must monitor stability (central differencing is neutrally stable and can cause oscillations at higher Pe numbers without artificial dissipation).
  * **Pressure Gradient:** On a collocated grid, the pressure gradient at cell *P* is approximated by neighboring cell pressures. A simple approach is linear interpolation: \$(\partial p/\partial x)\_P \approx (p\_E - p\_W)/(x\_E - x\_W)\$. Moukalled’s method integrates pressure over control-volume faces to derive this term, which for uniform grids reduces to central differencing. This central evaluation of \$\nabla p\$ in momentum ensures second-order accuracy for pressure force in momentum.
* **Linearization and Iteration:** The momentum equations are nonlinear (due to \$u·∇u\$). We linearize using a Picard approach: use the velocity from the previous iteration (or an explicit guess) in convective coefficients. This yields a linear system \$A\_u , u = b\$ for each velocity component per iteration. The resulting intermediate velocity field **u**\* conserves momentum by construction (it’s a solution of the discretized momentum with a given pressure field) but generally does not satisfy continuity (since the pressure was not yet adjusted for this iteration).
* **Boundary Conditions in Discrete Form:** For no-slip walls (including stationary walls and the moving lid), velocities at boundary faces are set to the wall velocity (Dirichlet condition). In matrix terms, these known values are applied by modifying the corresponding equations: the boundary cell’s momentum equation omits a neighbor contribution and moves it to the source term. For example, at the top lid for x-velocity, the north face convective term is \$\rho u\_{\text{lid}} A\_N\$ known, and diffusive term involves known \$u\_{\text{lid}}\$, which go into \$S\_P\$. This ensures the solved interior velocities honor the wall condition. All walls are impermeable, so no correction to pressure is needed at walls (pressure Neumann condition \$\partial p/\partial n = 0\$ is naturally satisfied if no normal velocity at wall).

**Implementation Aspects (Modular OOP Design):**

* **Code Structure:** The solver is written in a modular, object-oriented fashion for extensibility. For example:

  * A `Mesh` class encapsulates grid information (cell volumes, face areas/normals, neighbor connectivity, etc.) abstractly, whether the mesh is structured or unstructured.
  * Field classes (`Field` or specialized `ScalarField`/`VectorField`) hold variable values (pressure, velocity components) at cell centers, supporting operations like interpolation or gradient calculations.
  * A `FiniteVolumeSolver` (or separate `MomentumSolver`) class constructs the coefficient matrix for the momentum equations and applies boundary conditions, using mesh connectivity to assemble neighbor coefficients.
* **Assembly of Coefficients:** We loop over cells and their faces to assemble the sparse matrix for momentum. On each face connecting cell P to neighbor N, we add contributions:

  * Convective term: e.g. if face f has area \$A\_f\$ and unit normal pointing from P to N, convective coefficient \$a\_N\$ is \$\max(\rho (u·n)\_f, 0)\$ for upwind (this would multiply neighbor’s velocity in P’s equation). The own-cell coefficient \$a\_P\$ accumulates the outgoing convective flux \$\max(-\rho (u·n)\_f, 0)\$ for each face. This ensures mass conservation in convective transport.
  * Diffusive term: contribution \$a\_N = -\mu A\_f/d\_{PN}\$, \$a\_P\$ accumulates \$+\mu A\_f/d\_{PN}\$ (and similarly for the opposite face) ensuring symmetry. Here \$d\_{PN}\$ is distance between cell centers P and N projected on face normal.
  * Pressure term: treated explicitly in the source: \$b\_P\$ gets \$+V\_P (\partial p/\partial x)\_P\$ (since we bring \$-∇p\$ to RHS). Using current iteration’s pressure field \$p^{(n)}\$, we compute \$(\partial p/\partial x)\_P\$ by central differencing (or a least-squares gradient on unstructured meshes). This explicit source term drives the predicted velocity **u**\*.
* **Linear Solver for Momentum:** The momentum matrix is typically sparse and roughly diagonally dominant (especially with adequate under-relaxation). We can solve it with a sparse linear solver. In Python, one can use SciPy’s sparse LU or iterative solvers; however, for performance we prepare to interface with PETSc via `petsc4py`. The code assembles global coefficient arrays (rows, cols, values) that can be fed into a PETSc Mat object, enabling use of high-performance solvers (e.g. GMRES with ILU). For moderate grid sizes, an ILU-preconditioned BiCGSTAB from SciPy or PyAMG is sufficient, but PETSc integration makes the solver *future-proof* for large cases or parallel runs.
* **Numba Acceleration:** To mitigate Python loop overhead in assembly, critical loops are JIT-compiled with **Numba**. For example, the coefficient assembly and flux interpolation routines are decorated with `@njit`, allowing them to run at near C-speed on CPU. This is crucial for iterating over potentially tens of thousands of cells and faces. We ensure data structures (like neighbor index lists, flux arrays) are NumPy arrays or Python lists of simple types, which Numba can handle efficiently. In tests, Numba yields significant speed-ups for the assembly phase, making an all-Python FVM solver feasible for reasonably fine grids.
* **Collocated Face Interpolation:** On a collocated grid, when assembling the convective term, we need velocity values at face centers. We implement linear interpolation for faces: e.g. \$u\_f = (u\_P + u\_N)/2\$ for a face between cell P and N (or one-sided extrapolation for boundary faces). For upwinding, we determine the donor side by the sign of mass flux \$\rho (u·n)*f\$ computed from the previous iteration’s velocity field. This allows computing the face advected velocity and hence convective flux. The interpolation and upwind logic are encapsulated in a utility function (or within Numba loops) to keep solver code clean. For unstructured meshes, we use distance-weighted linear interpolation: \$u\_f = \frac{d*{PN}^N , u\_P + d\_{PN}^P , u\_N}{d\_{PN}}\$, where \$d\_{PN}^P\$ is the distance from face to cell P centroid, etc., ensuring consistency even if cells are irregular.
* **Diagonal Dominance:** After assembly, we check that each momentum equation’s diagonal coefficient \$a\_P\$ exceeds the sum of off-diagonals (\$\sum\_N a\_N\$). This is usually true due to physical diffusivity or added numerical diffusion from upwind. The code can assert this condition (minus any relaxation factor adjustments) as a sanity check for the discretization. Strong diagonal dominance guarantees the linear system is solvable and iterative solvers converge. If a diagonal is weaker than off-diagonals, the code logs a warning, as this could signal an under-resolved convective dominated flow or a coding error in coefficient calculation.
* **Modularity and OOP Benefits:** The object-oriented design means we can easily extend or swap components. For example, to test a different convection scheme, we implement a new interpolation function (e.g. `QUICKInterpolation`) and plug it into the assembly routine via a strategy pattern or configuration setting (without rewriting the entire solver). The `Mesh` abstraction allows the same momentum assembly code to work with a regular structured grid or a complex unstructured grid (only the mesh initialization differs). This modularity and abstraction are critical for maintainability and extensibility of a research-grade solver.

**Future Enhancements (Momentum Step):**

* **High-Order Convection Schemes:** Implementing **QUICK** (Quadratic Upwind Interpolation) or **TVD** schemes for convective fluxes can improve accuracy. QUICK uses quadratic interpolation through three points for flow-aligned upwind bias, reducing numerical diffusion (third-order accurate for uniform grids). TVD schemes (Total Variation Diminishing) with flux limiters can provide high resolution without spurious oscillations in regions with sharp gradients. These schemes require computing additional upstream values and limiters, which would integrate well into the solver’s interpolation module. For instance, one could add a limiter function to blend second-order central and first-order upwind values based on local gradient (e.g. MINMOD or SUPERBEE limiter) to ensure monotonicity.
* **Turbulence Modeling:** Although the current solver is laminar, in future a turbulence model (like a two-equation RANS model) could be added modularly. The momentum assembly can be extended to include turbulent viscosity in the diffusion term or extra source terms. The solver’s object-oriented structure allows adding a `TurbulenceModel` component that computes eddy viscosity and modifies the momentum equation coefficients accordingly.
* **Alternative Solvers:** Instead of solving each momentum component separately, a coupled approach or block-solve of the momentum system could be explored (especially for strongly coupled flows at high Reynolds). While SIMPLE typically treats them segregated, advanced usage might solve the u- and v-velocity equations together as a vector system. Preparing for this, one could interface with PETSc’s block matrix capabilities. However, this increases complexity and is usually not necessary for SIMPLE at moderate speeds.
* **Matrix-Free and GPU Offload:** Currently we explicitly assemble matrices. In the future, a matrix-free approach (avoiding forming large matrices and computing action on the fly) could be considered for better memory use, especially if combining with GPUs. For example, one could offload the linear solver to GPU by providing a custom mat-vec that computes fluxes on GPU for a guess vector. In the nearer term, using PETSc’s GPU support is feasible: we can assemble the sparse matrix on CPU (with Numba) and then transfer it to GPU memory for solving. This leverages GPU-accelerated linear algebra (e.g. using cuSOLVER or AmgX via PETSc) to speed up the momentum solves, which is beneficial for very fine grids or 3D cases. The code’s modular solver interface can make this a runtime option (e.g. `use_gpu_solver: true` in config triggers PETSc to use a GPU backend for solves).

## Pressure Correction Equation and Continuity Enforcement

**Mathematical Considerations:**

* **Derivation of Pressure-Correction Equation:** After obtaining the intermediate velocity field **u**\* from the momentum solve, we have a momentum-consistent flow that may violate continuity by some divergence \$\nabla·u^*\$. We introduce a pressure correction \$p'\$ and velocity correction **u'** such that the true solution is \$p = p^{(n)} + p'\$ and **\$u = u^* + u'$\*\*, where iteration \$(n)\$ is the old field. The goal is to choose \$p'\$ so that the corrected velocity **u** satisfies continuity. Substituting **u** and \$p\$ into the discrete continuity equation (mass balance in each cell) yields a Poisson-like equation for \$p'\`. In continuous form, this derivation (dropping higher-order terms) leads to \*\*\$\nabla \cdot \left(\frac{V\_P}{a\_P}\nabla p'\right) = \nabla \cdot u^*$\*\*. Here \$a\_P\$ is the diagonal coefficient from momentum (essentially the effective resistance to changing velocity in cell *P*), and \$V\_P\$ is the cell volume. This equation is analogous to Poisson’s equation \$\nabla^2 p' = \text{source}\$, with a coefficient \$\frac{V\_P}{a\_P}\$ that varies by cell. It ensures that the divergence of the corrected velocity field is zero: the source term \$\nabla·u^*\$ is the negative of the mass imbalance.
* **Physical Interpretation:** The pressure correction equation enforces mass conservation by adjusting pressure such that it drives the velocity corrections **u'** to cancel out any net flow imbalance in each cell. It’s derived by combining the momentum and continuity equations – effectively forming the discrete *Schur complement* for pressure. Solving for \$p'\$ is equivalent to ensuring that, when added, the new pressure field will yield divergence-free flow. This approach is necessary because the original system lacks a standalone pressure equation.
* **Rhie–Chow Interpolation:** A central issue on collocated grids is that a naive formulation of the pressure equation can lead to oscillatory (checkerboard) pressure and velocity fields (because pressure and velocity unknowns at the same locations allow a decoupled zigzag mode). The Rhie–Chow momentum interpolation method augments the face mass flux computation with a pressure correction term to avoid this decoupling. In practice, when evaluating \$\nabla \cdot u^*\$ for the pressure equation, instead of simple linear interpolation of **u*** at faces, we compute the face interim velocity as:
  $u_f^* = \bar{u}_f^* - D_f \, (\nabla p^{(n)})_f,$
  where \$\bar{u}*f^\*\$ is a basic interpolated face velocity (e.g. average of neighbor cell velocities) and the second term subtracts a pressure term. \$D\_f\$ is a coefficient related to the momentum matrix inverse (essentially \$D\_f = \frac{A\_f d*{PN}}{a\_P + a\_N}\$ for face connecting cells P and N, akin to the \$(V/a)\_f\$ term). This formula (from Rhie & Chow’s 1983 method) effectively adds a small artificial dissipation that couples pressure and velocity, eliminating the spuriously independent pressure field. Moukalled et al. describe this as computing face mass flow with the Rhie–Chow interpolation such that if the flow were fully converged (mass balanced), the pressure correction term vanishes. Thus, Rhie–Chow ensures that any pressure oscillation would induce a compensating velocity perturbation, damping the oscillation.
* **Discrete Equation (Coefficients):** The discretized pressure correction equation for each cell P arises from summing the flux corrections over its faces = negative of mass imbalance. It takes the form:
  $a_P' p'_P = \sum_{N} a_{N}' p'_N + b_P',$
  which is a sparse linear system similar to a Poisson equation. Here \$a\_N'\$ comes from the \$D\_f\$ coefficients at P’s faces: for a face between P and neighbor N, \$a\_P'\$ gets a contribution \$+\frac{\rho^2 A\_f^2}{a\_P^{\text{face}}}\$ and \$a\_N'\$ gets \$-\frac{\rho^2 A\_f^2}{a\_P^{\text{face}}}\$, reflecting \$(V/a)\_f\$ difference stencil. The source \$b\_P'\$ is \$\sum\_f \rho A\_f u\_f^\*\$, the net mass flux imbalance in cell P (with sign such that a positive net outflux produces a negative \$b\_P'\$ demanding a drop in pressure). On structured grids, this results in the classic five-point stencil for pressure: each interior cell’s \$p'\$ connects to its four neighbors. On unstructured meshes, the stencil connects to all neighboring cells that share a face.
* **Boundary Conditions for \$p'\$:** In a closed cavity with impermeable walls, the physically correct condition is zero normal velocity at the wall. The pressure correction equation uses Neumann boundary conditions: \$\partial p'/\partial n = 0\$ at walls, meaning no pressure correction flux across solid boundaries (since we don’t expect mass flow through walls). In implementation, for a wall face, we do not include a ghost cell; instead the pressure correction equation for the adjacent interior cell P is formed considering that face as having zero flux contribution – effectively \$a\_P'\$ gets the term for that face added and no neighbor. Because all boundaries are closed, the pressure correction equation has a singularity (pressure is determined only up to a constant). We handle this by fixing the pressure of one reference cell (e.g. set one corner cell’s \$p'\$ = 0 each iteration) or by removing the nullspace via setting the mean of \$p'\$ to zero after solve. This prevents the solver from drifting in the nullspace of adding a constant to pressure.

**Implementation Aspects:**

* **Matrix Assembly:** We assemble the pressure correction matrix by looping over faces of each cell, similar to assembling a diffusion operator. For an interior face between cell P and N with area \$A\_f\$ and normal from P to N:

  * Compute the face interpolation coefficient \$D\_f = \frac{V\_P}{a\_{P}} \frac{1}{d\_{PN}}\$ (if using a simple approach, or the more exact formula involving both P and N momentum coeffs: \$\frac{A\_f d\_{PN}}{a\_P + a\_N}\$ which is equivalent under uniform conditions). This \$D\_f\$ corresponds to \$(V/a)\_f\$ in the derivation.
  * Add to matrix: \$a\_P' ;+= \rho A\_f^2 D\_f\$ and \$a\_N' ;+= \rho A\_f^2 D\_f\$ (for symmetric contribution), and enter off-diagonals: \$- \rho A\_f^2 D\_f\$ in positions (P,N) and (N,P). (On a structured orthonormal grid, \$D\_f = \Delta x / a\_P\$ and this reduces to the familiar \$\frac{1}{a\_P}\$ factor in the stencil.)
  * For boundary faces (wall), we skip neighbor addition and simply add the face’s contribution to \$a\_P'\$ (Neumann BC effectively).
  * We build arrays of coefficients for each equation. The storage can reuse the mesh’s face list: for each face, if it has two neighbors P and N, we append entries: `(row=P, col=N, value=-coef)` and `(row=P, col=P, value=+coef)` etc., accumulating duplicates. This is well-suited to a **CSR (Compressed Sparse Row)** matrix format assembly.
* **Source Term (RHS) Assembly:** Concurrently, compute the mass flux imbalance \$b\_P'\$ for each cell P: \$b\_P' = -\sum\_{f \in P} \rho (\mathbf{u}^\* \cdot \mathbf{n})\_f A\_f\$. The face normal is taken positive outward of cell P. We calculate \$(\mathbf{u}^\* \cdot \mathbf{n})\_f\$ using the Rhie–Chow formula:

  * Determine an interim face velocity \$u\_f^*\$ as described: interpolate cell-centered \$u^*\$ to the face and subtract \$D\_f (\nabla p^{(n)})\_f\$ (the pressure gradient from cells P and N). For example, \$(\nabla p)*f \approx (p\_N^{(n)} - p\_P^{(n)})/d*{PN}\$ along the normal, and \$D\_f\$ as above.
  * Then \$(\mathbf{u}^\* \cdot \mathbf{n})\_f = u\_f^\* \cdot n\$ (for scalar normal velocity since in a 2D cavity, face normal is either horizontal or vertical). Multiply by \$\rho A\_f\$ to get mass flux \$m\_f^\*\$. Sum these for each cell (with sign convention: outflux positive, influx negative) to get the net imbalance.
  * This \$b\_P'\$ is placed in the RHS of the linear system. If the flow were perfectly divergent-free, each \$b\_P'\$ would be zero.
* **Linear Solver for Pressure Correction:** The pressure correction matrix is symmetric positive-(semi)definite (SPD) for an incompressible interior flow (the nullspace being the constant vector). We use a solver appropriate for SPD systems, like Conjugate Gradient with an algebraic multigrid preconditioner. PETSc offers efficient multigrid (GAMG) or direct solvers; we can use `petsc4py` to solve this system with one call after assembly. For smaller cases, SciPy’s CG or even sparse LU can be used, but multigrid is recommended as it scales better for finer grids (classic CFD codes use geometric multigrid for pressure-Poisson equations). Our code is structured to allow either: a `PressureSolver` class can abstract the solver choice. For instance, it can default to SciPy, but if a configuration flag is set, initialize a PETSc KSP solver with PC = hypre or GAMG. This modular solver backend approach makes it easy to offload to GPU or parallel in the future without changing the higher-level algorithm.
* **Stabilization and Under-Relaxation:** In SIMPLE, one often under-relaxes pressure updates to smooth convergence. We implement this by scaling the computed \$p'\$ before applying it: \$p := p^{(n)} + \alpha\_p p'\$. Another subtlety: some implementations retain some under-relaxation in the pressure correction equation itself (SIMPLEC algorithm modifies coefficients to improve convergence). Here, we stick to standard SIMPLE: we solve the unmodified pressure correction equation, then apply a relaxation factor \$\alpha\_p\$ in the update step. The code handles this by allowing \$\alpha\_p < 1\$ (from the YAML input) – if \$\alpha\_p=0.5\$, we simply half the p’ values before adding to the old pressure. This does not change the linear system, only the update.
* **Avoiding Singularities:** To handle the nullspace, the implementation pins one cell’s pressure correction to zero. We do this by setting that equation’s coefficients such that \$a\_{ref} = 1, b\_{ref}=0\$ and all off-diagonals on that row = 0 (Dirichlet condition for p’ at reference cell). This removes the singular degree of freedom. Alternatively, we could enforce \$\sum p' = 0\$ after solving (subtract the mean), which is equivalent. We ensure this is done every iteration. The chosen reference point does not affect velocity (only the absolute pressure level), which is inconsequential in incompressible flow.
* **Checkerboard Pressure Avoidance:** We validate that the Rhie–Chow implementation is effective by monitoring the pressure field for odd-even oscillations. The code can compute the difference in pressure between neighboring cells in a checkerboard pattern and ensure it decays. We can also compare the results with and without Rhie–Chow in a test mode to confirm that without it, a checkerboard pattern appears (this is a unit test strategy discussed later). Ensuring the Rhie–Chow terms are correctly implemented is crucial for collocated mesh stability. We follow Moukalled’s guideline that the mass flux for pressure correction uses the momentum interpolation so that a conservative mass flux field yields zero RHS in the pressure equation.

**Future Enhancements (Pressure & Coupling):**

* **PISO Algorithm (Transient Extension):** The SIMPLE algorithm as described is for steady-state (iterative until convergence). For unsteady flows, the **PISO** (Pressure Implicit with Splitting of Operators) algorithm can be employed. PISO can be seen as one SIMPLE iteration followed by one or more additional **pressure-correction (PRIME)** steps within each time step. After the first pressure correction (like SIMPLE), PISO immediately recomputes face fluxes and solves a second pressure correction to further reduce continuity error before advancing in time. This eliminates the need for iterative cycles at each time step, making it more efficient for transient simulations. Our solver is designed to be extensible to PISO: we can incorporate a loop over multiple pressure corrections per time-step. The collocated PISO sequence would be:

  1. Solve momentum implicitly to get **u**\* at new time step.
  2. Apply Rhie–Chow and solve pressure correction to get \$p'\_1\$ and correct to **u**\${}^{\*\*}\$, **p**\${}^{\*}\$.
  3. Optionally repeat: update fluxes and solve a second pressure correction equation for \$p'\_2\$, correct **u** and **p** again.
  4. Proceed to next time step with these corrected fields.
     By incorporating PISO, the code can handle transient lid-driven cavity cases or flows around complex geometries without outer iterations, yet maintain stability. Notably, PISO is essentially SIMPLE + additional correctors, so our modular solver can reuse the pressure correction solver in a loop. We just need to adapt the time-stepping driver to support multiple corrector steps.
* **SIMPLEC and SIMPLER:** Other SIMPLE-family algorithms (which could be future options) include SIMPLEC (SIMPLE-Consistent) and SIMPLER (SIMPLE-Revised). SIMPLEC modifies the pressure correction equation coefficients to improve convergence (essentially removing the slight approximations introduced by SIMPLE in the velocity correction equation), and SIMPLER introduces a separate “pressure equation” solved before momentum to supply a better initial guess. Our code structure (especially how we assemble and solve the pressure equation) could be adapted to these variants. For instance, SIMPLEC would require adjusting the \$D\_f\$ coefficients in the Rhie–Chow step to those recommended by the SIMPLEC derivation. These enhancements could yield faster convergence in certain cases and are worth considering as configuration options in the future.
* **Multi-Grid and Domain Decomposition:** As the pressure Poisson equation is often the slowest to converge, advanced solvers like multi-grid or domain decomposition could be integrated. PETSc makes it straightforward to experiment with geometric multigrid if we supply grid hierarchy or use algebraic multigrid (BoomerAMG/Hypre) as a black-box. In a parallel context, PETSc’s scalable solvers could solve the pressure system in parallel across MPI processes. The code’s mesh abstraction and use of PETSc means that moving to a distributed mesh (with ghost cells and partitioned matrix assembly) is an achievable extension. This would allow tackling very large 3D cases or high-resolution 2D cases beyond single-core memory limits.
* **Moving and Complex Geometry:** Future extensions might include support for moving boundaries or free surfaces (which would add a moving mesh or a volume-of-fluid equation). While not trivial, the current pressure-velocity coupling can be extended to those scenarios. For example, for moving mesh, one would add geometric conservation law adjustments to continuity and include mesh motion flux in the pressure equation source. Designing the code with clear separation of geometric data makes it easier to incorporate such changes.
* **Coupling with Energy/Species Equations:** Though beyond the current scope, the solver can be extended to solve additional scalar transport equations (temperature, etc.) once the pressure-velocity coupling is working. Since the code is object-oriented, adding another equation would involve adding a new field and equation assembly similar to momentum. The pressure coupling would remain unchanged (for constant-density flows). This highlights the importance of the solver’s extensibility for multi-physics in a research context.

## Velocity and Pressure Field Update (Correction Step)

**Mathematical Considerations:**

* **Velocity Correction:** Once we solve for the pressure correction \$p'\$ in the above step, we update the velocity at cell centers. The derivation gives a direct formula for velocity correction: **\$u' = -\frac{V}{a\_P}\nabla p'\$** (for each momentum equation). In practice, for each cell P, we compute the gradient of \$p'\$ (e.g. \$((p'\_E - p'\_W)/\Delta x,\ (p'\_N - p'*S)/\Delta y)\$ in 2D) and multiply by \$-\frac{V\_P}{a*{P}}\$ (which was our \$D\_P\$ term) to get **u’** at that cell. This is essentially the discrete form of \$u' = - D (∇p')\$ from the momentum correction equation. By construction, adding **u’** to **u**\* will cancel the divergence: the continuity equation for the corrected field is now satisfied up to linearization error.
* **Pressure Update:** The new pressure field is obtained by \$p^{(new)} = p^{(old)} + p'\$. The pressure correction is not under any differential operator in its own equation (it was solved as a potential), so we simply add it to get the adjusted pressure that ensures momentum and continuity are in balance. After this update, the momentum equation for the next iteration will use this new pressure.
* **Under-Relaxation Effects:** If under-relaxation is used (say \$\alpha\_p < 1\$), effectively the applied correction is \$α\_p p'\$. This means we intentionally do not fully close the mass balance in one iteration, which slows convergence but stabilizes. Mathematically, this can be seen as solving a slightly modified pressure equation (or doing a weighted update), but it’s easier viewed as a relaxation. The momentum equation in the next iteration will see a partially corrected pressure, and further iterations gradually drive the error to zero.
* **Convergence Criteria:** At convergence, the pressure correction \$p'\$ should be nearly zero everywhere (no further change in pressure), and velocity correction **u’** likewise near zero. This implies the continuity residual is near zero and the momentum equation residual (difference between left and right hand side) is within tolerance. We typically monitor:

  * Maximum or L2 norm of continuity error (e.g. \$\max | \nabla·u^\* |\$ before correction, or equivalently max \$|p'|\$ since they are linked).
  * Momentum residual for u and v (e.g. sum of absolute residuals in each equation).
    If these fall below set tolerances (e.g. \$10^{-6}\$), we declare the solution converged and stop iterating. In a steady solution, the final corrected velocity and pressure fields are the solution; in a transient PISO case, we proceed to the next time step using these as the starting point.

**Implementation Aspects:**

* **Computing Velocity Correction:** After solving for \$p'\$ (as an array for all cells), we loop over cells to compute corrections for each velocity component. For a structured mesh, this is straightforward using neighbor indices:

  * For interior cell P, \$u'*x\[P] = -\frac{V\_P}{a*{P,x}} \frac{(p'*{E}-p'*{W})}{2\Delta x}\$, and \$u'*y\[P] = -\frac{V\_P}{a*{P,y}} \frac{(p'*{N}-p'*{S})}{2\Delta y}\$. Here \$a\_{P,x}\$ is the diagonal coeff from the x-momentum eq for cell P (and similarly \$a\_{P,y}\$ for y-mom). We use the corresponding momentum coefficient to get the appropriate scaling in each direction. If the mesh is non-uniform or unstructured, we use the vector form: compute gradient \$\nabla p'\_P\$ via a loop over faces of P (sum \$p'\_N - p'*P\$ times face normal times \$A\_f/V\_P\$) – essentially a finite-volume gradient. Then apply \$-\frac{V\_P}{a*{P}}\$ times that gradient for each component of **u’**. We use the respective \$a\_P\$ from each momentum component’s matrix, but if the discretization was homogeneous (same stencil weights), those are nearly equal.
  * The velocity correction calculation is a good candidate for Numba acceleration, as it’s a simple loop over cells with arithmetic. We implement it as a function `applyPressureCorrection(p_prime)` that updates the internal velocity arrays.
* **Updating Fields:** We maintain arrays for the velocity components and pressure. After computing **u’**, we do:

  ```python
  u[:] = u_star + u_prime
  v[:] = v_star + v_prime
  p[:] = p_old + alpha_p * p_prime
  ```

  If \$\alpha\_p < 1\$, we scale \$p'\$ accordingly before adding. The intermediate fields (**u**\*, old **p**) are stored from the previous steps. We then also recompute face velocities \$u\_f\$ for each face (if needed for next iteration’s flux computations) using the corrected cell velocities – this is part of preparing the next iteration.
* **Boundary Conditions on Correction:** Velocity corrections at boundaries should ideally be zero (since no penetration at walls remains no penetration even after correction). Our formulation inherently yields zero normal velocity correction at walls because the continuity violation for a wall-adjacent cell is interior. We explicitly enforce that by setting \$u'\_n = 0\$ at solid boundaries (and tangential correction can also be taken as zero to keep the imposed no-slip). In practice, if the interior solve tried to induce a slight slip at the wall, we override it because the wall velocity is fixed. Pressure correction at boundaries with Neumann condition should result in near-zero \$p'\$ at walls (since \$\partial p'/\partial n =0\$ implies no difference between wall ghost and interior, effectively \$p'\$ in ghost = \$p'\$ in cell, so the cell’s value doesn’t change from that condition). We don’t need to update pressure at a wall because it’s not explicitly stored; only cell pressures exist in collocated FVM. The reference pressure cell remains fixed (its \$p'\$ was zero by our matrix modification), so its pressure stays as reference (e.g. 0) after update.
* **Post-Update Checks:** After updating, we recompute the residuals. For momentum, we can plug the updated velocities and pressures into the discrete equations to calculate the imbalance. For continuity, we compute \$\nabla·u\$ for each cell with the new **u**. These serve as diagnostics. The solver prints or logs the max residuals each iteration. We also keep track of iteration count. If it exceeds a maximum (from config) without convergence, the solver stops and reports lack of convergence.
* **Data Management:** To avoid confusion, we typically use arrays for current iteration and previous. For example, we might have `p` for current pressure and `p_old` for last iteration’s pressure if needed. However, an efficient approach is to update in place with under-relaxation: `p += alpha_p * p_prime` uses the old p on left and updates it. This way we don’t need a full copy of the field each iteration (saving memory). The same for velocity: if momentum solve was implicit, we solved directly for **u**\* given old p, so we have **u**\* as new. We can then do `u = u_star + u_prime` in place for each component array.
* **Under-Relaxation Implementation:** We handle velocity under-relaxation inside the momentum solver by modifying coefficients (classic approach). For pressure, as mentioned, we handle it in the update. The code reads relaxation factors from the YAML configuration (e.g. `relaxation: {u: 0.7, p: 0.3}`) and applies them accordingly. It’s important to treat these carefully to not bias the solution: for instance, if \$\alpha\_p < 1\$, the pressure correction equation’s RHS should technically be scaled by α as well to ensure we don’t overshoot mass correction. Simpler practice: solve for full \$p'\$ but add partially. This can leave a small residual which subsequent iterations clear.
* **Stopping Criteria:** The YAML may specify tolerances like `absTol` or `relTol` for momentum and continuity. We check (for example) the L∞ norm of continuity error (max net mass flux in any cell divided by reference flow) and the L2 norm of momentum residual. If below thresholds, break. We also can use the magnitude of \$p'\$ as an indicator: when the pressure corrections become very small compared to pressure values, the solution is converged.
* **Logging and Debugging:** The update step is where we can insert checks for conservation: sum of mass imbalance should be zero (global continuity), or check that pressure didn’t diverge. We also verify that applying correction actually reduced the continuity error significantly as expected. For debugging, one might output intermediate fields at each iteration to ensure the algorithm is proceeding correctly (e.g. print a few sample velocities). In an object-oriented design, the update step might be a method like `SimpleSolver.applyCorrections()` that also handles these diagnostics and calls the residual calculator.

**Future Enhancements (Correction Step):**

* **Adaptive Under-Relaxation:** Instead of fixed relaxation factors, a future improvement is to use adaptive relaxation. For example, the code could monitor residual reduction and adjust \$\alpha\_u, \alpha\_p\$ per iteration (increasing when convergence is smooth, decreasing if residuals stagnate or oscillate). This can accelerate convergence. Techniques like Aitken relaxation or residual norm monitoring can be used for adaptation.
* **High-Fidelity Boundary Conditions:** In more complex cases, one might need to update boundary pressures (e.g. at an open outlet, pressure is fixed or follows an external condition rather than zero-gradient). In a lid-driven cavity we had all walls, but for generalization, implementing pressure outlets (Dirichlet p) or specifying mass flow outlets (which impose an integral constraint on \$p'\$) could be added. The correction step would then include adjustments at those boundaries (for instance, if p is fixed at an outlet, we set \$p'=0\$ there every iteration as a Dirichlet condition in the pressure solve).
* **Convergence Acceleration:** To reach steady-state faster, one could combine SIMPLE with transient under-relaxation (sometimes called False Transient or Artificial Compressibility method). Essentially, instead of pure iteration, add a pseudo-time derivative \$\frac{\partial u}{\partial \tau} = \text{residual}\$ and integrate in pseudo-time. This can sometimes converge faster than standard SIMPLE for difficult cases. Our solver could incorporate this as an alternate mode. After each correction step, instead of directly iterating, one could march a small time step in pseudo-time using the corrected velocities as starting point. This is an advanced strategy but worth noting for research exploration.
* **Result Extrapolation:** Another acceleration method is to extrapolate the solution between iterations (Steffensen or Anderson acceleration). The code can store a history of a few past iterates and attempt to extrapolate a better guess for the next iteration. This has been shown to speed up convergence of fixed-point iterations. As a future feature, implementing Anderson acceleration for the SIMPLE outer iteration could reduce iteration counts without altering the core solver. The modular structure (treating the whole SIMPLE iteration as a fixed-point on (u,p)) would allow plugging in such an accelerator on top of the iteration loop.

## Unit Testing and Verification Strategies

* **Manufactured Solutions:** One robust way to test the solver is the Method of Manufactured Solutions. Impose an analytical solution for **u(x,y)** and **p(x,y)** (even an arbitrary polynomial) and derive source terms that make them an exact solution of modified equations. Then run the solver with those sources. This allows verification of spatial accuracy: for example, if we choose a linear velocity field, the solver’s numerical gradient and divergence calculations should reproduce it with machine precision (for linear fields, a second-order scheme has zero truncation error). This tests **gradient exactness** – e.g., set \$u(x,y) = ax + b\$, \$v(x,y)=cy+d\$, and constant pressure; the computed pressure gradient and divergence should be exactly constant and zero respectively. We ensure the code yields zero continuity residual and recovers the imposed pressure gradient in momentum to floating-point precision, confirming no consistency errors in interpolation or differentiation.
* **Diagonal Dominance and Coefficient Checks:** As a unit test for the discretization, for a given mesh and parameters, assemble the momentum and pressure matrices and examine their coefficients. We check that each row’s diagonal entry is greater than the sum of absolute off-diagonals (ensuring an M-matrix for diffusion-dominated cases). Additionally, all off-diagonal coefficients should be non-positive (for diffusion and pressure Poisson equations) or have the correct sign pattern for convection (upwind convection yields negative off-diagonals for inflow neighbors, zero for outflow neighbors). These properties guarantee stability (e.g., a positive coefficient matrix for pressure-Poisson). We can automate a test that flags any coefficient sign irregularities or negative diagonal, which might indicate a bug in assembly.
* **Continuity Satisfaction (Closed Domain):** We can test that for a simple incompressible case (e.g. solid body rotation or a trivial static flow), the solver does not introduce spurious divergence. For instance, initialize a uniform flow that already satisfies continuity and has a corresponding pressure gradient (or no pressure gradient). The solver should ideally recognize it as converged (zero corrections). If any pressure correction is generated, it indicates a potential issue in the Rhie–Chow implementation or boundary condition handling. This kind of test validates that a divergence-free initial field remains divergence-free.
* **Rhie–Chow Validation:** To specifically test the Rhie–Chow interpolation, one can construct a scenario prone to checkerboarding. A common test is a 2D grid with alternating high/low pressure values in a checkerboard pattern and zero velocity initially. Without Rhie–Chow, this pressure field would produce zero net force in momentum (because gradients cancel out at cell centers) and remain unchanged, while velocities oscillate arbitrarily – an unstable situation. With Rhie–Chow, the pressure oscillation should induce corrections that damp it out. A simpler test: run the solver on a coarse grid without Rhie–Chow and observe if pressure oscillates cell-to-cell; then turn on Rhie–Chow and confirm the pressure field smooths out. Additionally, we ensure that in a 1D steady pipe flow scenario (pressure-driven), the Rhie–Chow term does not impede the correct linear pressure drop and uniform velocity (i.e., it should introduce no bias when the solution is smooth). This ensures the momentum interpolation is consistency with a true physical solution (Rhie–Chow adds only a small fourth-order dissipation that vanishes for smooth fields).
* **Lid-Driven Cavity Benchmark:** Although not a unit test per se, comparing the solver’s output for lid-driven cavity against known benchmark data (Ghia et al. 1982 classic results for Re=100, 1000) is a validation step. We can incorporate this as an automated test: compute the centerline velocity profiles and compare to published values. Ensure the relative error is within acceptable range and that refining the mesh reduces the error (demonstrating convergence). This tests the integrated performance of the whole algorithm in a realistic setting.
* **Matrix Solvers and Linear Algebra Tests:** We also unit-test the linear solver integration. For example, assemble a known SPD matrix (like a simple Laplacian) and test that our PETSc solver interface or SciPy solver returns the correct solution for a known RHS. Since we rely on external libraries for solving, this is mainly to ensure our assembly and data transfer to those libraries are correct. We might also include a test where we start with a known pressure field, compute fluxes, solve pressure correction, and see that it yields zero correction if the field was already divergence-free (testing the correctness of source term assembly).
* **Code Structure Tests:** On the software side, the mesh abstraction can be tested by using a very simple mesh (like 2x2 cells) and manually computing what the coefficients should be. We then compare the solver’s assembled matrices to expected values. This kind of test on a small grid can catch indexing errors and ensure the mesh connectivity is used properly (especially test on an unstructured tiny mesh, like a single quadrilateral split into two triangles, to see if the solver handles non-orthogonal neighbors correctly).
* **Performance Regression:** As we add optimizations (Numba, etc.), we include tests to ensure the numerical results remain the same. A practical approach is to run a short simulation with and without Numba and confirm the residual history and final result are identical (within round-off differences). This ensures that the optimized code path hasn’t altered the logic. We also test that enabling PETSc vs using a native solver yields the same solution for a given tolerance.

## Mesh Abstraction and FVM Mesh Design

* **Mesh-Agnostic Data Structures:** The solver is built on a mesh abstraction so it can handle structured grids (like uniform Cartesian for the cavity) as well as unstructured meshes (e.g. triangulated or arbitrary polyhedral cells) without changes to the core algorithms. The `Mesh` class provides arrays or lists for:

  * **Cells:** each cell has an index, volume, centroid coordinates, and lists of its faces.
  * **Faces:** each face knows the indices of the cells it separates (left and right cell, or one cell and a boundary identifier), the face area (in 2D, length) and unit normal vector (pointing in a consistent direction, say from the “left” cell to “right” cell). It may also store the distance between cell centroids (for interpolation).
  * **Neighbors connectivity:** from the face data we derive neighbor lists: for each cell, which cells are its neighbors and via which face. This is useful for assembling equations and applying boundary conditions.
* **Handling Structured vs Unstructured:** For a structured Cartesian mesh, we can generate the connectivity implicitly (by i,j indexing), but our solver *uses only the abstract connectivity*. This means even if the mesh is logically structured, we fill the Mesh data structure (cells and faces) and then treat it generically. This approach was guided by Moukalled’s emphasis on general finite volume formulation that does not depend on alignment or regular connectivity. For example, the gradient of a scalar at cell P is computed by summing contributions from all faces of P, rather than assuming alignment with axes. This works for any cell shape. The code does not hard-code a 5-point stencil; it derives the stencil from the neighbor connectivity at runtime.
* **Boundary Representation:** Boundaries are treated as special faces with one neighboring cell and one fictitious outside neighbor. The Mesh can hold a list of boundary faces grouped by boundary condition type (e.g. all lid faces in a group with prescribed velocity). Each face knows if it’s a boundary and carries an outward normal and area. This uniform treatment allows the solver to loop over all faces (interior and boundary) in flux calculations. Boundary conditions are applied by injecting appropriate values (for Dirichlet) or zero-gradient assumptions (for Neumann) when computing fluxes or assembling matrices. For instance, when assembling pressure equation, if a face is a boundary, we simply omit the neighbor contribution and handle the flux as described above.
* **Grid Independence:** The mesh abstraction makes the solver *grid independent*: it doesn’t matter if the cells are squares, rectangles, or arbitrary polyhedra; as long as volume and face info is provided, the conservation laws apply the same. This is crucial for extending to complex geometries (like curvilinear meshes around obstacles). It also eases implementing adaptive mesh refinement or non-uniform grids – the solver logic remains unchanged, only the mesh input changes.
* **Mesh Input/Output:** We foresee reading mesh data from external sources (e.g. a Gmsh `.msh` file or other CAD-generated mesh). The Mesh class can include a function to load from a file (parsing nodes and element connectivity) or convert from a structured grid generator. By isolating this, we ensure the solver doesn’t need to know file formats. Output can be handled similarly: writing cell-centered fields to VTK or CSV files for visualization. The solver likely includes a `Writer` utility that, given a Mesh and field data, produces an output file (for example, a VTK unstructured grid file listing cell vertices and cell data for pressure/velocity).
* **Parallel and Domain Decomposition Considerations:** Although initially we run in serial, the mesh abstraction can be extended to support domain decomposition by splitting cells among processes and treating inter-process boundaries similar to physical boundaries (with ghost cells). Using PETSc’s parallel matrix assembly would naturally fit if the mesh can provide a partitioning. Planning this ahead, we ensure no part of the solver explicitly assumes global indexing that isn’t easily remapped. For now, everything is indexed in a single array, but we design methods that would allow injecting ghost cell values for a boundary condition, etc., which parallels how one would do in MPI.
* **Geometric Calculations:** The mesh class is responsible for computing geometric factors such as face normals and areas, cell volumes, etc., especially for unstructured meshes. In a structured orthogonal grid, these are trivial (normals align with axes, areas = Δx or Δy, volume = ΔxΔy). In a general mesh, the code may need to calculate the centroid of a polygon, area of an arbitrary polygon face, etc. We rely on standard formulas (e.g. polygon area via shoelace formula, centroid via averaging vertices) as part of mesh preprocessing. By doing this once at mesh load, we avoid recomputing geometry in inner loops.
* **Example (Structured vs. Unstructured):** To illustrate mesh abstraction: a \$50\times50\$ structured square grid and an unstructured mesh of an L-shaped domain would both produce a `Mesh` with (say) 2500 cells. The solver assembling the momentum equation will iterate over \~2500 cells and each cell’s faces. In the structured case each interior cell has 4 faces (4 neighbors), in the unstructured maybe some cells have 5 or 6 if polygons. But since the loop doesn’t assume exactly 4 neighbors, it naturally assembles whatever neighbors exist. This flexibility is built-in by design. It realizes the finite volume method’s strength of handling arbitrary polyhedral control volumes.
* **Testing the Mesh Module:** We include simple mesh creation functions (like `make_uniform_grid(nx, ny, Lx, Ly)`) to generate structured meshes for testing. For unstructured, we might include a couple of hard-coded small meshes (like two triangles making a square) to verify correctness. By swapping these in and running a single SIMPLE iteration (or checking flux calculations), we ensure the solver’s mesh handling is robust.

## YAML Configuration for Simulation Control

* **Configuration File Use:** The solver reads a YAML file to configure simulation parameters and case setup. This externalizes all tunable settings from the code, enhancing usability and experiment reproducibility. A typical YAML config (e.g. `cavity.yaml`) might include sections for **mesh**, **fluid properties**, **numerical schemes**, **solver settings**, and **output**:

  ```yaml
  mesh:
    type: structured
    nx: 128
    ny: 128
    length: 1.0    # cavity domain size
  fluid:
    density: 1.0
    viscosity: 0.01   # (Re = 100 if lid velocity = 1)
  numerics:
    convection_scheme: "Upwind"  # or "Central", "QUICK"
    relaxation:
      u: 0.7
      p: 0.3
    tolerance:
      momentum: 1e-6
      continuity: 1e-6
    max_iterations: 5000
  solver:
    linear_solver: "petsc"   # could be "scipy" or "petsc"
    preconditioner: "ilu"
  output:
    interval: 100
    format: "vtk"
  ```

  This example illustrates how key aspects are specified in a human-readable way. The solver will parse this and set up the simulation accordingly.
* **Mesh Specification:** The YAML can either define a simple structured mesh via parameters (nx, ny, etc.) or provide a filename for an unstructured mesh. For instance, `mesh: file: "geometry.msh"`. The Mesh class can interpret these; if a file is given, call the appropriate reader, if parameters given, generate on the fly. This flexibility allows quickly switching geometries without code changes.
* **Solver Parameters:** Under **numerics** and **solver**, we include anything that influences the numerical algorithm:

  * Choice of convection differencing scheme (so one can test Upwind vs QUICK by editing YAML, not code).
  * Relaxation factors and iteration limits.
  * Tolerances for convergence.
  * Option to choose which linear algebra backend to use (e.g., a flag to use PETSc vs a built-in solver, or to enable/disables Rhie–Chow for testing).
  * Possibly time-step size and total time for unsteady runs (if we extend to transient).
* **Read/Write Implementation:** We use a YAML parsing library (like PyYAML) at program start. It populates a Python dict or a lightweight custom config object. The solver classes then query this config. For example, the `MomentumSolver` might do:

  ```python
  scheme = config["numerics"]["convection_scheme"]
  if scheme == "Upwind": self.interpolation = upwind_interpolation
  elif scheme == "QUICK": self.interpolation = quick_interpolation
  ```

  Similarly, relaxation factors are read and stored. The advantage is clear: to run a new scenario or adjust parameters, the user edits the YAML, not the code. This also means we can run parameter sweeps or optimizations by generating different YAML files for each run.
* **Simulation Control:** The YAML can define start and end conditions for a run. E.g., number of iterations or a convergence criterion. The main driver can use these to stop the iteration loop. For transient cases, one would have `time_start`, `time_end`, `dt`. The solver loop would then integrate in time steps, using PISO or iterative SIMPLE per step as configured.
* **Output Settings:** The YAML can include output controls: how frequently to write solution snapshots, what format (VTK, CSV, etc.), and which fields to output (maybe velocity vectors, pressure, vorticity, etc.). This decouples the output logic from solver logic. For example, if `output: interval: 100`, the solver will output every 100 iterations. If a transient, maybe every N time steps or at specified times. This gives the user control to balance disk usage and analysis needs.
* **Reusability and Reproducibility:** Having all parameters in YAML means one can exactly reproduce a run by using the same YAML. It also allows keeping a log of what settings were used. When writing results, we often include a copy of the YAML in the output folder for record. Researchers can tweak one parameter at a time in the YAML to isolate effects (e.g. turning on a new scheme).
* **Extensibility:** As we develop new features (say a turbulence model or a new boundary condition type), we can extend the YAML schema to include those (perhaps under a new section `physics:` or an option under numerics). The parser can provide defaults if new options are missing for backward compatibility. This way, older YAML files still work with newer code (with default behavior for new options), and new capabilities are just new keys in the config.
* **Error Handling:** The code validates the YAML input, providing clear errors if something is mis-specified (unknown scheme name, etc.). This helps users identify typos in the config. We also ensure units or nondimensional parameters are understood (for instance, if Reynolds number is given instead of viscosity, the code could accept that too).
* **Example – Lid-Driven Cavity Control:** For our lid-driven cavity, a YAML might specify the lid velocity implicitly via boundary conditions:

  ```yaml
  boundaries:
    top:
      u: 1.0
      v: 0.0
    other_walls:
      u: 0.0
      v: 0.0
  ```

  Or simply assume top wall has unit velocity as part of the problem definition (like above fluid viscosity yields Re given lid velocity =1 and domain length =1). We can incorporate boundary conditions either in mesh (marking boundaries with names) or in a separate section. The solver reads these and sets the boundary condition values accordingly in the momentum assembly.
* **Integration with Execution Scripts:** The YAML approach also means we can write a Python script or use a command-line to run the solver with a given YAML: e.g. `python run_solver.py --config cavity.yaml`. This makes it easy to run batch jobs or integrate into a larger pipeline (like testing multiple mesh resolutions, etc., by generating multiple YAMLs). The solver itself can be written to just take a config dictionary, so it’s easily callable from notebooks or other scripts as well, increasing flexibility for research usage.

## Conclusion

In summary, the Python-based SIMPLE solver is designed following Moukalled’s finite volume guidelines to ensure both mathematical rigor and implementation flexibility. Each step of the SIMPLE algorithm – momentum prediction, pressure correction, and field updates – has been detailed in terms of discretization assumptions and coding practices. The solver’s modular structure (with mesh abstraction, field objects, and solver classes) makes it extensible to more advanced algorithms like PISO for unsteady flow, higher-order advection schemes, and even GPU-accelerated computation. Comprehensive testing (from unit tests of flux calculations to benchmarking against known solutions) will ensure the solver’s accuracy and reliability. This high-fidelity approach provides a solid foundation for simulating incompressible flows in complex geometries, while remaining adaptable for future research needs and enhancements, bridging the gap between educational code and a development-ready CFD tool.
